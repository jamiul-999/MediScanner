{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:48:39.892321Z",
     "iopub.status.busy": "2025-10-23T05:48:39.891600Z",
     "iopub.status.idle": "2025-10-23T05:48:43.908568Z",
     "shell.execute_reply": "2025-10-23T05:48:43.907565Z",
     "shell.execute_reply.started": "2025-10-23T05:48:39.892296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:49:14.186229Z",
     "iopub.status.busy": "2025-10-23T05:49:14.185489Z",
     "iopub.status.idle": "2025-10-23T05:49:14.192019Z",
     "shell.execute_reply": "2025-10-23T05:49:14.191264Z",
     "shell.execute_reply.started": "2025-10-23T05:49:14.186197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and memory optimization for Kaggle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:50:04.615086Z",
     "iopub.status.busy": "2025-10-23T05:50:04.614802Z",
     "iopub.status.idle": "2025-10-23T05:50:04.619961Z",
     "shell.execute_reply": "2025-10-23T05:50:04.619172Z",
     "shell.execute_reply.started": "2025-10-23T05:50:04.615066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chest-xrays-indiana-university /kaggle/input/chest-xrays-indiana-university/images/images_normalized /kaggle/input/chest-xrays-indiana-university/indiana_reports.csv /kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\n"
     ]
    }
   ],
   "source": [
    "KAGGLE_INPUT_PATH = '/kaggle/input/chest-xrays-indiana-university'\n",
    "IMAGES_PATH = os.path.join(KAGGLE_INPUT_PATH, 'images', 'images_normalized')\n",
    "REPORTS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_reports.csv')\n",
    "PROJECTIONS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_projections.csv')\n",
    "\n",
    "print(KAGGLE_INPUT_PATH, IMAGES_PATH, REPORTS_PATH, PROJECTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:50:47.357229Z",
     "iopub.status.busy": "2025-10-23T05:50:47.356448Z",
     "iopub.status.idle": "2025-10-23T05:50:47.363524Z",
     "shell.execute_reply": "2025-10-23T05:50:47.362783Z",
     "shell.execute_reply.started": "2025-10-23T05:50:47.357170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Low-Rank Adaptation layer for efficient fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=32):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original transformation\n",
    "        result = torch.matmul(x, self.weight.t())\n",
    "        \n",
    "        # Add LoRA adaptation\n",
    "        lora_output = torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)\n",
    "        result += lora_output * self.scaling\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:51:13.396792Z",
     "iopub.status.busy": "2025-10-23T05:51:13.396207Z",
     "iopub.status.idle": "2025-10-23T05:51:13.402692Z",
     "shell.execute_reply": "2025-10-23T05:51:13.401924Z",
     "shell.execute_reply.started": "2025-10-23T05:51:13.396768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    \"\"\"Custom dataset for X-ray images and reports\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, reports, tokenizer, max_length=256, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.reports = reports\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and process image\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            # Return a black image if loading fails\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Process text\n",
    "        report = str(self.reports[idx])\n",
    "        encoded = self.tokenizer(\n",
    "            report,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "            'report': report\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction ResNet(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:51:33.370470Z",
     "iopub.status.busy": "2025-10-23T05:51:33.369731Z",
     "iopub.status.idle": "2025-10-23T05:51:33.376139Z",
     "shell.execute_reply": "2025-10-23T05:51:33.375301Z",
     "shell.execute_reply.started": "2025-10-23T05:51:33.370447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"CNN Encoder using ResNet for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size=768, use_lora=False):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        # Use ResNet18 for memory efficiency on Kaggle\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        if use_lora:\n",
    "            self.linear = LoRALayer(resnet.fc.in_features, embed_size)\n",
    "        else:\n",
    "            self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.linear(features)\n",
    "        features = self.bn(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:52:06.634707Z",
     "iopub.status.busy": "2025-10-23T05:52:06.634055Z",
     "iopub.status.idle": "2025-10-23T05:52:06.643230Z",
     "shell.execute_reply": "2025-10-23T05:52:06.642475Z",
     "shell.execute_reply.started": "2025-10-23T05:52:06.634681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayReportGenerator(nn.Module):\n",
    "    \"\"\"Main model combining CNN and RoBERTa with optional LoRA\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size=768, hidden_size=768, use_lora=False):\n",
    "        super(XRayReportGenerator, self).__init__()\n",
    "        \n",
    "        self.use_lora = use_lora\n",
    "        \n",
    "        # Image encoder\n",
    "        self.encoder = CNNEncoder(embed_size, use_lora=use_lora)\n",
    "        \n",
    "        # Text decoder using RoBERTa\n",
    "        config = RobertaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=6,  # Reduced for memory\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=2048,\n",
    "            max_position_embeddings=256\n",
    "        )\n",
    "        self.roberta = RobertaModel(config)\n",
    "        \n",
    "        # Projection layers with optional LoRA\n",
    "        if use_lora:\n",
    "            self.image_projection = LoRALayer(embed_size, hidden_size)\n",
    "            self.output_projection = LoRALayer(hidden_size, vocab_size)\n",
    "        else:\n",
    "            self.image_projection = nn.Linear(embed_size, hidden_size)\n",
    "            self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Freeze RoBERTa layers if using LoRA\n",
    "        if use_lora:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        # Encode images\n",
    "        image_features = self.encoder(images)\n",
    "        image_features = self.image_projection(image_features)\n",
    "        image_features = image_features.unsqueeze(1)  # Add sequence dimension\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            # Training mode: use teacher forcing\n",
    "            text_embeddings = self.roberta.embeddings(input_ids)\n",
    "            \n",
    "            # Concatenate image features with text embeddings\n",
    "            combined_embeddings = torch.cat([image_features, text_embeddings], dim=1)\n",
    "            \n",
    "            # Create attention mask for combined sequence\n",
    "            batch_size = images.size(0)\n",
    "            image_mask = torch.ones(batch_size, 1).to(images.device)\n",
    "            if attention_mask is not None:\n",
    "                combined_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
    "            else:\n",
    "                combined_mask = image_mask\n",
    "            \n",
    "            # Pass through RoBERTa\n",
    "            outputs = self.roberta(\n",
    "                inputs_embeds=combined_embeddings,\n",
    "                attention_mask=combined_mask\n",
    "            )\n",
    "            \n",
    "            # Project to vocabulary\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = self.dropout(hidden_states)\n",
    "            logits = self.output_projection(hidden_states)\n",
    "            \n",
    "            return logits[:, 1:, :]  # Remove image feature from output\n",
    "        else:\n",
    "            # Inference mode\n",
    "            return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference class for generating report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T05:53:13.617494Z",
     "iopub.status.busy": "2025-10-23T05:53:13.616892Z",
     "iopub.status.idle": "2025-10-23T05:53:13.625821Z",
     "shell.execute_reply": "2025-10-23T05:53:13.624908Z",
     "shell.execute_reply.started": "2025-10-23T05:53:13.617468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayReportGenerator(nn.Module):\n",
    "    \"\"\"Main model combining CNN and RoBERTa with optional LoRA\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size=768, hidden_size=768, use_lora=False):\n",
    "        super(XRayReportGenerator, self).__init__()\n",
    "        \n",
    "        self.use_lora = use_lora\n",
    "        \n",
    "        # Image encoder\n",
    "        self.encoder = CNNEncoder(embed_size, use_lora=use_lora)\n",
    "        \n",
    "        # Text decoder using RoBERTa\n",
    "        config = RobertaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=6,  # Reduced for memory\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=2048,\n",
    "            max_position_embeddings=256\n",
    "        )\n",
    "        self.roberta = RobertaModel(config)\n",
    "        \n",
    "        # Projection layers with optional LoRA\n",
    "        if use_lora:\n",
    "            self.image_projection = LoRALayer(embed_size, hidden_size)\n",
    "            self.output_projection = LoRALayer(hidden_size, vocab_size)\n",
    "        else:\n",
    "            self.image_projection = nn.Linear(embed_size, hidden_size)\n",
    "            self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Freeze RoBERTa layers if using LoRA\n",
    "        if use_lora:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        # Encode images\n",
    "        image_features = self.encoder(images)\n",
    "        image_features = self.image_projection(image_features)\n",
    "        image_features = image_features.unsqueeze(1)  # Add sequence dimension\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            # Training mode: use teacher forcing\n",
    "            text_embeddings = self.roberta.embeddings(input_ids)\n",
    "            \n",
    "            # Concatenate image features with text embeddings\n",
    "            combined_embeddings = torch.cat([image_features, text_embeddings], dim=1)\n",
    "            \n",
    "            # Create attention mask for combined sequence\n",
    "            batch_size = images.size(0)\n",
    "            image_mask = torch.ones(batch_size, 1).to(images.device)\n",
    "            if attention_mask is not None:\n",
    "                combined_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
    "            else:\n",
    "                combined_mask = image_mask\n",
    "            \n",
    "            # Pass through RoBERTa\n",
    "            outputs = self.roberta(\n",
    "                inputs_embeds=combined_embeddings,\n",
    "                attention_mask=combined_mask\n",
    "            )\n",
    "            \n",
    "            # Project to vocabulary\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = self.dropout(hidden_states)\n",
    "            logits = self.output_projection(hidden_states)\n",
    "            \n",
    "            return logits[:, 1:, :]  # Remove image feature from output\n",
    "        else:\n",
    "            # Inference mode\n",
    "            return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 516716,
     "sourceId": 951996,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
