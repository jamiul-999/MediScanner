{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge nltk\n!pip install -q transformers==4.35.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T20:52:07.392696Z","iopub.execute_input":"2025-10-23T20:52:07.393269Z","iopub.status.idle":"2025-10-23T20:52:21.766565Z","shell.execute_reply.started":"2025-10-23T20:52:07.393246Z","shell.execute_reply":"2025-10-23T20:52:21.765939Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2025.9.18)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Imports and Device Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge import Rouge\nimport warnings\n\nimport sys\n\n# Patch transformers BEFORE importing\nimport importlib.util\nspec = importlib.util.find_spec(\"transformers\")\nif spec:\n    import transformers.utils.hub\n    transformers.utils.hub.list_repo_templates = lambda *args, **kwargs: []\n    print(\"‚úÖ Transformers patched successfully\")\nwarnings.filterwarnings('ignore')\n\n# Set device and memory optimization for Kaggle\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T20:19:42.308895Z","iopub.execute_input":"2025-10-23T20:19:42.309174Z","iopub.status.idle":"2025-10-23T20:19:42.315542Z","shell.execute_reply.started":"2025-10-23T20:19:42.309155Z","shell.execute_reply":"2025-10-23T20:19:42.314810Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Transformers patched successfully\nUsing device: cuda\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Kaggle Dataset paths","metadata":{}},{"cell_type":"code","source":"KAGGLE_INPUT_PATH = '/kaggle/input/chest-xrays-indiana-university'\nIMAGES_PATH = os.path.join(KAGGLE_INPUT_PATH, 'images', 'images_normalized')\nREPORTS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_reports.csv')\nPROJECTIONS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_projections.csv')\n\nprint(KAGGLE_INPUT_PATH) \nprint(IMAGES_PATH)\nprint(REPORTS_PATH)\nprint(PROJECTIONS_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:43:27.848521Z","iopub.execute_input":"2025-10-23T19:43:27.848806Z","iopub.status.idle":"2025-10-23T19:43:27.903714Z","shell.execute_reply.started":"2025-10-23T19:43:27.848788Z","shell.execute_reply":"2025-10-23T19:43:27.902815Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chest-xrays-indiana-university\n/kaggle/input/chest-xrays-indiana-university/images/images_normalized\n/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\n/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nEMBED_SIZE = 512  # Reduced from 768\nHIDDEN_SIZE = 512\nMAX_LENGTH = 128  # Reduced from 256\nNUM_EPOCHS = 10\nLEARNING_RATE = 1e-4\nUSE_LORA = True  # Use LoRA for memory efficiency","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:43:32.540439Z","iopub.execute_input":"2025-10-23T19:43:32.541077Z","iopub.status.idle":"2025-10-23T19:43:32.545054Z","shell.execute_reply.started":"2025-10-23T19:43:32.541054Z","shell.execute_reply":"2025-10-23T19:43:32.544062Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## LoRA Implementation","metadata":{}},{"cell_type":"code","source":"class LoRALayer(nn.Module):\n    \"\"\"Low-Rank Adaptation layer for efficient fine-tuning\"\"\"\n    \n    def __init__(self, in_features, out_features, rank=16, alpha=32):\n        super().__init__()\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank\n        \n        # LoRA parameters\n        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n        \n        # Freeze original weights\n        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n        \n    def forward(self, x):\n        # Original transformation\n        result = torch.matmul(x, self.weight.t())\n        \n        # Add LoRA adaptation\n        lora_output = torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)\n        result += lora_output * self.scaling\n        \n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:43:34.021972Z","iopub.execute_input":"2025-10-23T19:43:34.022370Z","iopub.status.idle":"2025-10-23T19:43:34.030333Z","shell.execute_reply.started":"2025-10-23T19:43:34.022337Z","shell.execute_reply":"2025-10-23T19:43:34.029452Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Dataset Setup","metadata":{}},{"cell_type":"code","source":"class XRayDataset(Dataset):\n    \"\"\"Custom dataset for X-ray images and reports\"\"\"\n    \n    def __init__(self, image_paths, reports, tokenizer, max_length=256, transform=None):\n        self.image_paths = image_paths\n        self.reports = reports\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load and process image\n        image_path = self.image_paths[idx]\n        try:\n            image = Image.open(image_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n        except:\n            # Return a black image if loading fails\n            image = torch.zeros(3, 224, 224)\n        \n        # Process text\n        report = str(self.reports[idx])\n        encoded = self.tokenizer(\n            report,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoded['input_ids'].squeeze(),\n            'attention_mask': encoded['attention_mask'].squeeze(),\n            'report': report\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:43:38.846253Z","iopub.execute_input":"2025-10-23T19:43:38.846886Z","iopub.status.idle":"2025-10-23T19:43:38.855766Z","shell.execute_reply.started":"2025-10-23T19:43:38.846855Z","shell.execute_reply":"2025-10-23T19:43:38.854888Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Feature Extraction ResNet(CNN)","metadata":{}},{"cell_type":"code","source":"class CNNEncoder(nn.Module):\n    \"\"\"CNN Encoder using ResNet for feature extraction\"\"\"\n    \n    def __init__(self, embed_size=768, use_lora=False):\n        super(CNNEncoder, self).__init__()\n        # Use ResNet18 for memory efficiency on Kaggle\n        resnet = models.resnet18(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        \n        if use_lora:\n            self.linear = LoRALayer(resnet.fc.in_features, embed_size)\n        else:\n            self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n            \n        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n        \n    def forward(self, images):\n        with torch.no_grad():\n            features = self.resnet(images)\n        features = features.reshape(features.size(0), -1)\n        features = self.linear(features)\n        features = self.bn(features)\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:43:42.632101Z","iopub.execute_input":"2025-10-23T19:43:42.632381Z","iopub.status.idle":"2025-10-23T19:43:42.638486Z","shell.execute_reply.started":"2025-10-23T19:43:42.632360Z","shell.execute_reply":"2025-10-23T19:43:42.637469Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Adding RoBERTa","metadata":{}},{"cell_type":"code","source":"class XRayReportGenerator(nn.Module):\n    \"\"\"Main model combining CNN and RoBERTa with optional LoRA\"\"\"\n    \n    def __init__(self, vocab_size, embed_size=512, hidden_size=512, use_lora=False):\n        super(XRayReportGenerator, self).__init__()\n        \n        self.use_lora = use_lora\n        \n        # Image encoder\n        self.encoder = CNNEncoder(embed_size, use_lora=use_lora)\n        \n        # Text decoder using RoBERTa\n        config = RobertaConfig(\n            vocab_size=vocab_size,\n            hidden_size=hidden_size,\n            num_hidden_layers=3,  # Reduced for memory\n            num_attention_heads=8,\n            intermediate_size=1024,\n            max_position_embeddings=128\n        )\n        self.roberta = RobertaModel(config)\n        \n        # Projection layers with optional LoRA\n        if use_lora:\n            self.image_projection = LoRALayer(embed_size, hidden_size)\n            self.output_projection = LoRALayer(hidden_size, vocab_size)\n        else:\n            self.image_projection = nn.Linear(embed_size, hidden_size)\n            self.output_projection = nn.Linear(hidden_size, vocab_size)\n            \n        self.dropout = nn.Dropout(0.1)\n        \n        # Freeze RoBERTa layers if using LoRA\n        if use_lora:\n            for param in self.roberta.parameters():\n                param.requires_grad = False\n        \n    def forward(self, images, input_ids=None, attention_mask=None):\n        # Encode images\n        image_features = self.encoder(images)\n        image_features = self.image_projection(image_features)\n        image_features = image_features.unsqueeze(1)  # Add sequence dimension\n        \n        if input_ids is not None:\n            # Training mode: use teacher forcing\n            text_embeddings = self.roberta.embeddings(input_ids)\n            \n            # Concatenate image features with text embeddings\n            combined_embeddings = torch.cat([image_features, text_embeddings], dim=1)\n            \n            # Create attention mask for combined sequence\n            batch_size = images.size(0)\n            image_mask = torch.ones(batch_size, 1).to(images.device)\n            if attention_mask is not None:\n                combined_mask = torch.cat([image_mask, attention_mask], dim=1)\n            else:\n                combined_mask = image_mask\n            \n            # Pass through RoBERTa\n            outputs = self.roberta(\n                inputs_embeds=combined_embeddings,\n                attention_mask=combined_mask\n            )\n            \n            # Project to vocabulary\n            hidden_states = outputs.last_hidden_state\n            hidden_states = self.dropout(hidden_states)\n            logits = self.output_projection(hidden_states)\n            \n            return logits[:, 1:, :]  # Remove image feature from output\n        else:\n            # Inference mode\n            return image_features\n\n    def generate_report(self, image, tokenizer, max_length=100, temperature=0.7):\n        \"\"\"Generate report from a single image\"\"\"\n        self.eval()\n        with torch.no_grad():\n            if image.dim() == 3:\n                image = image.unsqueeze(0)\n            image = image.to(device)\n            \n            # Start with BOS token\n            generated = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n            \n            for _ in range(max_length):\n                attention_mask = torch.ones_like(generated)\n                \n                logits = self.forward(image, generated, attention_mask)\n                next_token_logits = logits[0, -1, :] / temperature\n                \n                # Greedy decoding\n                next_token = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n                \n                generated = torch.cat([generated, next_token], dim=1)\n                \n                if next_token.item() == tokenizer.eos_token_id:\n                    break\n            \n            report = tokenizer.decode(generated[0], skip_special_tokens=True)\n            return report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:46:04.963295Z","iopub.execute_input":"2025-10-23T19:46:04.963583Z","iopub.status.idle":"2025-10-23T19:46:04.974623Z","shell.execute_reply.started":"2025-10-23T19:46:04.963565Z","shell.execute_reply":"2025-10-23T19:46:04.973822Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"def prepare_data():\n    print(\"Loading data...\")\n    reports_df = pd.read_csv(REPORTS_PATH)\n    projections_df = pd.read_csv(PROJECTIONS_PATH)\n    \n    # Merge and clean\n    merged_df = projections_df.merge(reports_df, on='uid', how='inner')\n    merged_df = merged_df.dropna(subset=['findings', 'filename'])\n    \n    # Filter for frontal views only\n    merged_df = merged_df[merged_df['projection'].isin(['Frontal', 'AP', 'PA'])]\n    \n    # Create full image paths\n    merged_df['image_path'] = merged_df['filename'].apply(\n        lambda x: os.path.join(IMAGES_PATH, x)\n    )\n    \n    # Keep only existing images\n    merged_df = merged_df[merged_df['image_path'].apply(os.path.exists)]\n    \n    print(f\"Total samples found: {len(merged_df)}\")\n    \n    # Limit dataset for faster training on Kaggle\n    merged_df = merged_df.head(1000)\n    print(f\"Using {len(merged_df)} samples for training\")\n    \n    return merged_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:47:10.864197Z","iopub.execute_input":"2025-10-23T19:47:10.864463Z","iopub.status.idle":"2025-10-23T19:47:10.870670Z","shell.execute_reply.started":"2025-10-23T19:47:10.864442Z","shell.execute_reply":"2025-10-23T19:47:10.869630Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Training function","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion, tokenizer, epoch):\n    model.train()\n    total_loss = 0\n    \n    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n    for batch_idx, batch in enumerate(pbar):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits = model(images, input_ids, attention_mask)\n        \n        # Shift for next token prediction\n        logits = logits[:, :-1, :].contiguous()\n        targets = input_ids[:, 1:].contiguous()\n        \n        # Calculate loss\n        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        # Clear cache periodically\n        if batch_idx % 10 == 0:\n            torch.cuda.empty_cache()\n    \n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:47:32.841992Z","iopub.execute_input":"2025-10-23T19:47:32.842685Z","iopub.status.idle":"2025-10-23T19:47:32.851828Z","shell.execute_reply.started":"2025-10-23T19:47:32.842630Z","shell.execute_reply":"2025-10-23T19:47:32.851061Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Evaluation function","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, tokenizer, num_samples=20):\n    \"\"\"Evaluate on limited samples for speed\"\"\"\n    model.eval()\n    bleu_scores = []\n    rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n    rouge_evaluator = Rouge()\n    smoothing = SmoothingFunction().method1\n    \n    sample_count = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            images = batch['image'].to(device)\n            reference_reports = batch['report']\n            \n            for i, image in enumerate(images):\n                if sample_count >= num_samples:\n                    break\n                    \n                generated_report = model.generate_report(image, tokenizer, max_length=50)\n                reference = reference_reports[i]\n                \n                # BLEU score\n                reference_tokens = reference.lower().split()\n                generated_tokens = generated_report.lower().split()\n                \n                if len(generated_tokens) > 0:\n                    bleu = sentence_bleu([reference_tokens], generated_tokens, \n                                        smoothing_function=smoothing)\n                    bleu_scores.append(bleu)\n                \n                # ROUGE scores\n                try:\n                    if generated_report.strip() and reference.strip():\n                        rouge_result = rouge_evaluator.get_scores(generated_report, reference)[0]\n                        rouge_scores['rouge-1'].append(rouge_result['rouge-1']['f'])\n                        rouge_scores['rouge-2'].append(rouge_result['rouge-2']['f'])\n                        rouge_scores['rouge-l'].append(rouge_result['rouge-l']['f'])\n                except:\n                    pass\n                \n                sample_count += 1\n            \n            if sample_count >= num_samples:\n                break\n    \n    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n    avg_rouge1 = np.mean(rouge_scores['rouge-1']) if rouge_scores['rouge-1'] else 0\n    avg_rouge2 = np.mean(rouge_scores['rouge-2']) if rouge_scores['rouge-2'] else 0\n    avg_rougel = np.mean(rouge_scores['rouge-l']) if rouge_scores['rouge-l'] else 0\n    \n    return {\n        'BLEU': avg_bleu,\n        'ROUGE-1': avg_rouge1,\n        'ROUGE-2': avg_rouge2,\n        'ROUGE-L': avg_rougel\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:47:46.585360Z","iopub.execute_input":"2025-10-23T19:47:46.585975Z","iopub.status.idle":"2025-10-23T19:47:46.593958Z","shell.execute_reply.started":"2025-10-23T19:47:46.585950Z","shell.execute_reply":"2025-10-23T19:47:46.592978Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"def main():\n    print(\"=\"*60)\n    print(\"X-Ray Report Generator - Training Pipeline\")\n    print(\"=\"*60)\n    \n    # Prepare data\n    df = prepare_data()\n    \n    # Split data\n    train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n    \n    print(f\"\\nüìä Dataset Split:\")\n    print(f\"   Train samples: {len(train_df)}\")\n    print(f\"   Val samples:   {len(val_df)}\")\n    \n    # Initialize tokenizer - WORKAROUND for chat template bug\n    print(\"\\nüîß Loading tokenizer...\")\n    from transformers import AutoTokenizer, RobertaTokenizerFast\n    import transformers\n    \n    # Monkey patch to bypass the chat template bug\n    original_list_repo_templates = None\n    try:\n        from transformers.utils.hub import list_repo_templates\n        original_list_repo_templates = list_repo_templates\n        # Replace with a function that returns empty list\n        transformers.utils.hub.list_repo_templates = lambda *args, **kwargs: []\n    except:\n        pass\n    \n    tokenizer = None\n    \n    # Method 1: Try FacebookAI/roberta-base with patch\n    try:\n        tokenizer = RobertaTokenizerFast.from_pretrained('FacebookAI/roberta-base')\n        print(\"   ‚úÖ Loaded FacebookAI/roberta-base\")\n    except Exception as e1:\n        print(f\"   Method 1 failed: {str(e1)[:80]}...\")\n    \n    # Method 2: Try roberta-base with patch\n    if tokenizer is None:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n            print(\"   ‚úÖ Loaded roberta-base\")\n        except Exception as e2:\n            print(f\"   Method 2 failed: {str(e2)[:80]}...\")\n    \n    # Method 3: Try distilroberta-base with patch\n    if tokenizer is None:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', use_fast=True)\n            print(\"   ‚úÖ Loaded distilroberta-base\")\n        except Exception as e3:\n            print(f\"   Method 3 failed: {str(e3)[:80]}...\")\n    \n    # Method 4: Load from local cache if available\n    if tokenizer is None:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained('roberta-base', local_files_only=True)\n            print(\"   ‚úÖ Loaded from local cache\")\n        except Exception as e4:\n            print(f\"   Method 4 failed: {str(e4)[:80]}...\")\n    \n    # Restore original function\n    if original_list_repo_templates is not None:\n        transformers.utils.hub.list_repo_templates = original_list_repo_templates\n    \n    if tokenizer is None:\n        raise Exception(\"All tokenizer loading methods failed! Try upgrading transformers: pip install --upgrade transformers\")\n    \n    print(f\"   Vocab size: {tokenizer.vocab_size}\")\n    \n    # Data transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    print(\"\\nüì¶ Creating datasets...\")\n    train_dataset = XRayDataset(\n        train_df['image_path'].tolist(),\n        train_df['findings'].tolist(),\n        tokenizer,\n        max_length=MAX_LENGTH,\n        transform=transform\n    )\n    \n    val_dataset = XRayDataset(\n        val_df['image_path'].tolist(),\n        val_df['findings'].tolist(),\n        tokenizer,\n        max_length=MAX_LENGTH,\n        transform=transform\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True, \n        num_workers=2,\n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=4, \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # Initialize model\n    print(\"\\nüèóÔ∏è  Building model...\")\n    model = XRayReportGenerator(\n        vocab_size=tokenizer.vocab_size,\n        embed_size=EMBED_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        use_lora=USE_LORA\n    ).to(device)\n    \n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    \n    print(f\"   Total parameters:     {total_params:,}\")\n    print(f\"   Trainable parameters: {trainable_params:,}\")\n    print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n    \n    # Training setup\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n    \n    # Training loop\n    print(\"\\n\" + \"=\"*60)\n    print(\"Starting Training\")\n    print(\"=\"*60)\n    \n    best_bleu = 0\n    best_epoch = 0\n    \n    for epoch in range(1, NUM_EPOCHS + 1):\n        print(f\"\\nüìà Epoch {epoch}/{NUM_EPOCHS}\")\n        print(\"-\" * 60)\n        \n        # Train\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, tokenizer, epoch)\n        print(f\"   Average Train Loss: {train_loss:.4f}\")\n        \n        # Evaluate\n        print(\"\\n   Evaluating model...\")\n        metrics = evaluate_model(model, val_loader, tokenizer, num_samples=20)\n        \n        print(f\"\\n   üìä Validation Metrics:\")\n        print(f\"      BLEU Score:  {metrics['BLEU']:.4f}\")\n        print(f\"      ROUGE-1:     {metrics['ROUGE-1']:.4f}\")\n        print(f\"      ROUGE-2:     {metrics['ROUGE-2']:.4f}\")\n        print(f\"      ROUGE-L:     {metrics['ROUGE-L']:.4f}\")\n        \n        # Save best model\n        if metrics['BLEU'] > best_bleu:\n            best_bleu = metrics['BLEU']\n            best_epoch = epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'bleu': best_bleu,\n                'metrics': metrics\n            }, 'best_xray_model.pth')\n            print(f\"      ‚úÖ Best model saved! (BLEU: {best_bleu:.4f})\")\n        \n        scheduler.step()\n        \n        # Memory cleanup\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚ú® Training Complete!\")\n    print(\"=\"*60)\n    print(f\"Best BLEU Score: {best_bleu:.4f} (Epoch {best_epoch})\")\n    \n    return model, tokenizer, val_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:57:29.236093Z","iopub.execute_input":"2025-10-23T19:57:29.236501Z","iopub.status.idle":"2025-10-23T19:57:29.252157Z","shell.execute_reply.started":"2025-10-23T19:57:29.236477Z","shell.execute_reply":"2025-10-23T19:57:29.251398Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def generate_report_from_path(image_path, model, tokenizer, transform):\n    \"\"\"Generate report from image path\"\"\"\n    model.eval()\n    \n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image)\n    \n    # Generate report\n    report = model.generate_report(image, tokenizer, max_length=80)\n    \n    return report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:57:36.421605Z","iopub.execute_input":"2025-10-23T19:57:36.421851Z","iopub.status.idle":"2025-10-23T19:57:36.426718Z","shell.execute_reply.started":"2025-10-23T19:57:36.421829Z","shell.execute_reply":"2025-10-23T19:57:36.425947Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Demo Function","metadata":{}},{"cell_type":"code","source":"def demo_report_generation(model, tokenizer, val_df, transform, num_examples=3):\n    \"\"\"Show example predictions\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîç Demo: Generating Reports for Sample Images\")\n    print(\"=\"*60)\n    \n    model.eval()\n    \n    for i in range(min(num_examples, len(val_df))):\n        print(f\"\\n--- Example {i+1} ---\")\n        \n        image_path = val_df.iloc[i]['image_path']\n        actual_report = val_df.iloc[i]['findings']\n        \n        print(f\"Image: {os.path.basename(image_path)}\")\n        print(f\"\\nüìÑ Actual Report:\\n{actual_report[:200]}...\")\n        \n        generated_report = generate_report_from_path(image_path, model, tokenizer, transform)\n        print(f\"\\nü§ñ Generated Report:\\n{generated_report}\")\n        print(\"-\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T19:57:37.796570Z","iopub.execute_input":"2025-10-23T19:57:37.797256Z","iopub.status.idle":"2025-10-23T19:57:37.803946Z","shell.execute_reply.started":"2025-10-23T19:57:37.797222Z","shell.execute_reply":"2025-10-23T19:57:37.803200Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Run","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Train the model\n    model, tokenizer, val_df = main()\n    \n    # Create transform for inference\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Show demo predictions\n    demo_report_generation(model, tokenizer, val_df, transform, num_examples=3)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ Pipeline Complete!\")\n    print(\"=\"*60)\n    print(\"\\nüí° To generate a report for a new image, use:\")\n    print(\"   report = generate_report_from_path(image_path, model, tokenizer, transform)\")\n    print(\"\\nüìÅ Best model saved as: best_xray_model.pth\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T20:53:08.701018Z","iopub.execute_input":"2025-10-23T20:53:08.701749Z","iopub.status.idle":"2025-10-23T20:53:14.587234Z","shell.execute_reply.started":"2025-10-23T20:53:08.701721Z","shell.execute_reply":"2025-10-23T20:53:14.586218Z"}},"outputs":[{"name":"stdout","text":"============================================================\nX-Ray Report Generator - Training Pipeline\n============================================================\nLoading data...\nTotal samples found: 3307\nUsing 1000 samples for training\n\nüìä Dataset Split:\n   Train samples: 850\n   Val samples:   150\n\nüîß Loading tokenizer...\n   Method 1 failed: 404 Client Error. (Request ID: Root=1-68fa95b9-4a7b2eeb391bb44d5efd722d;7de6670f...\n   Method 2 failed: 404 Client Error. (Request ID: Root=1-68fa95b9-7de5e2181fa5ed2a2d214848;3afa9685...\n   Method 3 failed: 404 Client Error. (Request ID: Root=1-68fa95ba-73ccd714553496ef1b6f0127;03ba54a7...\n   Method 4 failed: expected str, bytes or os.PathLike object, not NoneType...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3720335299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create transform for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1501087307.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tokenizer loading methods failed! Try upgrading transformers: pip install --upgrade transformers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Vocab size: {tokenizer.vocab_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: All tokenizer loading methods failed! Try upgrading transformers: pip install --upgrade transformers"],"ename":"Exception","evalue":"All tokenizer loading methods failed! Try upgrading transformers: pip install --upgrade transformers","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}