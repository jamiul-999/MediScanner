{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge nltk\n",
    "!pip install -q transformers==4.35.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import warnings\n",
    "\n",
    "import sys\n",
    "\n",
    "# Patch transformers BEFORE importing\n",
    "import importlib.util\n",
    "spec = importlib.util.find_spec(\"transformers\")\n",
    "if spec:\n",
    "    import transformers.utils.hub\n",
    "    transformers.utils.hub.list_repo_templates = lambda *args, **kwargs: []\n",
    "    print(\"‚úÖ Transformers patched successfully\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and memory optimization for Kaggle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT_PATH = '/kaggle/input/chest-xrays-indiana-university'\n",
    "IMAGES_PATH = os.path.join(KAGGLE_INPUT_PATH, 'images', 'images_normalized')\n",
    "REPORTS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_reports.csv')\n",
    "PROJECTIONS_PATH = os.path.join(KAGGLE_INPUT_PATH, 'indiana_projections.csv')\n",
    "\n",
    "print(KAGGLE_INPUT_PATH) \n",
    "print(IMAGES_PATH)\n",
    "print(REPORTS_PATH)\n",
    "print(PROJECTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EMBED_SIZE = 512  # Reduced from 768\n",
    "HIDDEN_SIZE = 512\n",
    "MAX_LENGTH = 128  # Reduced from 256\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_LORA = True  # Use LoRA for memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Low-Rank Adaptation layer for efficient fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, rank=16, alpha=32):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA parameters\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original transformation\n",
    "        result = torch.matmul(x, self.weight.t())\n",
    "        \n",
    "        # Add LoRA adaptation\n",
    "        lora_output = torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)\n",
    "        result += lora_output * self.scaling\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayDataset(Dataset):\n",
    "    \"\"\"Custom dataset for X-ray images and reports\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, reports, tokenizer, max_length=256, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.reports = reports\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and process image\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            # Return a black image if loading fails\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Process text\n",
    "        report = str(self.reports[idx])\n",
    "        encoded = self.tokenizer(\n",
    "            report,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "            'report': report\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction ResNet(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"CNN Encoder using ResNet for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_size=768, use_lora=False):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        # Use ResNet18 for memory efficiency on Kaggle\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        if use_lora:\n",
    "            self.linear = LoRALayer(resnet.fc.in_features, embed_size)\n",
    "        else:\n",
    "            self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "            \n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.linear(features)\n",
    "        features = self.bn(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XRayReportGenerator(nn.Module):\n",
    "    \"\"\"Main model combining CNN and RoBERTa with optional LoRA\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=512, use_lora=False):\n",
    "        super(XRayReportGenerator, self).__init__()\n",
    "        \n",
    "        self.use_lora = use_lora\n",
    "        \n",
    "        # Image encoder\n",
    "        self.encoder = CNNEncoder(embed_size, use_lora=use_lora)\n",
    "        \n",
    "        # Text decoder using RoBERTa\n",
    "        config = RobertaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=3,  # Reduced for memory\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=1024,\n",
    "            max_position_embeddings=128\n",
    "        )\n",
    "        self.roberta = RobertaModel(config)\n",
    "        \n",
    "        # Projection layers with optional LoRA\n",
    "        if use_lora:\n",
    "            self.image_projection = LoRALayer(embed_size, hidden_size)\n",
    "            self.output_projection = LoRALayer(hidden_size, vocab_size)\n",
    "        else:\n",
    "            self.image_projection = nn.Linear(embed_size, hidden_size)\n",
    "            self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Freeze RoBERTa layers if using LoRA\n",
    "        if use_lora:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        # Encode images\n",
    "        image_features = self.encoder(images)\n",
    "        image_features = self.image_projection(image_features)\n",
    "        image_features = image_features.unsqueeze(1)  # Add sequence dimension\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            # Training mode: use teacher forcing\n",
    "            text_embeddings = self.roberta.embeddings(input_ids)\n",
    "            \n",
    "            # Concatenate image features with text embeddings\n",
    "            combined_embeddings = torch.cat([image_features, text_embeddings], dim=1)\n",
    "            \n",
    "            # Create attention mask for combined sequence\n",
    "            batch_size = images.size(0)\n",
    "            image_mask = torch.ones(batch_size, 1).to(images.device)\n",
    "            if attention_mask is not None:\n",
    "                combined_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
    "            else:\n",
    "                combined_mask = image_mask\n",
    "            \n",
    "            # Pass through RoBERTa\n",
    "            outputs = self.roberta(\n",
    "                inputs_embeds=combined_embeddings,\n",
    "                attention_mask=combined_mask\n",
    "            )\n",
    "            \n",
    "            # Project to vocabulary\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            hidden_states = self.dropout(hidden_states)\n",
    "            logits = self.output_projection(hidden_states)\n",
    "            \n",
    "            return logits[:, 1:, :]  # Remove image feature from output\n",
    "        else:\n",
    "            # Inference mode\n",
    "            return image_features\n",
    "\n",
    "    def generate_report(self, image, tokenizer, max_length=100, temperature=0.7):\n",
    "        \"\"\"Generate report from a single image\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if image.dim() == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "            \n",
    "            # Start with BOS token\n",
    "            generated = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                attention_mask = torch.ones_like(generated)\n",
    "                \n",
    "                logits = self.forward(image, generated, attention_mask)\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                \n",
    "                # Greedy decoding\n",
    "                next_token = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "            \n",
    "            report = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    print(\"Loading data...\")\n",
    "    reports_df = pd.read_csv(REPORTS_PATH)\n",
    "    projections_df = pd.read_csv(PROJECTIONS_PATH)\n",
    "    \n",
    "    # Merge and clean\n",
    "    merged_df = projections_df.merge(reports_df, on='uid', how='inner')\n",
    "    merged_df = merged_df.dropna(subset=['findings', 'filename'])\n",
    "    \n",
    "    # Filter for frontal views only\n",
    "    merged_df = merged_df[merged_df['projection'].isin(['Frontal', 'AP', 'PA'])]\n",
    "    \n",
    "    # Create full image paths\n",
    "    merged_df['image_path'] = merged_df['filename'].apply(\n",
    "        lambda x: os.path.join(IMAGES_PATH, x)\n",
    "    )\n",
    "    \n",
    "    # Keep only existing images\n",
    "    merged_df = merged_df[merged_df['image_path'].apply(os.path.exists)]\n",
    "    \n",
    "    print(f\"Total samples found: {len(merged_df)}\")\n",
    "    \n",
    "    # Limit dataset for faster training on Kaggle\n",
    "    merged_df = merged_df.head(1000)\n",
    "    print(f\"Using {len(merged_df)} samples for training\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, tokenizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift for next token prediction\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, tokenizer, num_samples=20):\n",
    "    \"\"\"Evaluate on limited samples for speed\"\"\"\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}\n",
    "    rouge_evaluator = Rouge()\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            reference_reports = batch['report']\n",
    "            \n",
    "            for i, image in enumerate(images):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                generated_report = model.generate_report(image, tokenizer, max_length=50)\n",
    "                reference = reference_reports[i]\n",
    "                \n",
    "                # BLEU score\n",
    "                reference_tokens = reference.lower().split()\n",
    "                generated_tokens = generated_report.lower().split()\n",
    "                \n",
    "                if len(generated_tokens) > 0:\n",
    "                    bleu = sentence_bleu([reference_tokens], generated_tokens, \n",
    "                                        smoothing_function=smoothing)\n",
    "                    bleu_scores.append(bleu)\n",
    "                \n",
    "                # ROUGE scores\n",
    "                try:\n",
    "                    if generated_report.strip() and reference.strip():\n",
    "                        rouge_result = rouge_evaluator.get_scores(generated_report, reference)[0]\n",
    "                        rouge_scores['rouge-1'].append(rouge_result['rouge-1']['f'])\n",
    "                        rouge_scores['rouge-2'].append(rouge_result['rouge-2']['f'])\n",
    "                        rouge_scores['rouge-l'].append(rouge_result['rouge-l']['f'])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                sample_count += 1\n",
    "            \n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "    \n",
    "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
    "    avg_rouge1 = np.mean(rouge_scores['rouge-1']) if rouge_scores['rouge-1'] else 0\n",
    "    avg_rouge2 = np.mean(rouge_scores['rouge-2']) if rouge_scores['rouge-2'] else 0\n",
    "    avg_rougel = np.mean(rouge_scores['rouge-l']) if rouge_scores['rouge-l'] else 0\n",
    "    \n",
    "    return {\n",
    "        'BLEU': avg_bleu,\n",
    "        'ROUGE-1': avg_rouge1,\n",
    "        'ROUGE-2': avg_rouge2,\n",
    "        'ROUGE-L': avg_rougel\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"X-Ray Report Generator - Training Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare data\n",
    "    df = prepare_data()\n",
    "    \n",
    "    # Split data\n",
    "    train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Split:\")\n",
    "    print(f\"   Train samples: {len(train_df)}\")\n",
    "    print(f\"   Val samples:   {len(val_df)}\")\n",
    "    \n",
    "    # Initialize tokenizer - WORKAROUND for chat template bug\n",
    "    print(\"\\nüîß Loading tokenizer...\")\n",
    "    from transformers import AutoTokenizer, RobertaTokenizerFast\n",
    "    import transformers\n",
    "    \n",
    "    # Monkey patch to bypass the chat template bug\n",
    "    original_list_repo_templates = None\n",
    "    try:\n",
    "        from transformers.utils.hub import list_repo_templates\n",
    "        original_list_repo_templates = list_repo_templates\n",
    "        # Replace with a function that returns empty list\n",
    "        transformers.utils.hub.list_repo_templates = lambda *args, **kwargs: []\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    tokenizer = None\n",
    "    \n",
    "    # Method 1: Try FacebookAI/roberta-base with patch\n",
    "    try:\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained('FacebookAI/roberta-base')\n",
    "        print(\"   ‚úÖ Loaded FacebookAI/roberta-base\")\n",
    "    except Exception as e1:\n",
    "        print(f\"   Method 1 failed: {str(e1)[:80]}...\")\n",
    "    \n",
    "    # Method 2: Try roberta-base with patch\n",
    "    if tokenizer is None:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)\n",
    "            print(\"   ‚úÖ Loaded roberta-base\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   Method 2 failed: {str(e2)[:80]}...\")\n",
    "    \n",
    "    # Method 3: Try distilroberta-base with patch\n",
    "    if tokenizer is None:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', use_fast=True)\n",
    "            print(\"   ‚úÖ Loaded distilroberta-base\")\n",
    "        except Exception as e3:\n",
    "            print(f\"   Method 3 failed: {str(e3)[:80]}...\")\n",
    "    \n",
    "    # Method 4: Load from local cache if available\n",
    "    if tokenizer is None:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained('roberta-base', local_files_only=True)\n",
    "            print(\"   ‚úÖ Loaded from local cache\")\n",
    "        except Exception as e4:\n",
    "            print(f\"   Method 4 failed: {str(e4)[:80]}...\")\n",
    "    \n",
    "    # Restore original function\n",
    "    if original_list_repo_templates is not None:\n",
    "        transformers.utils.hub.list_repo_templates = original_list_repo_templates\n",
    "    \n",
    "    if tokenizer is None:\n",
    "        raise Exception(\"All tokenizer loading methods failed! Try upgrading transformers: pip install --upgrade transformers\")\n",
    "    \n",
    "    print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\nüì¶ Creating datasets...\")\n",
    "    train_dataset = XRayDataset(\n",
    "        train_df['image_path'].tolist(),\n",
    "        train_df['findings'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length=MAX_LENGTH,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = XRayDataset(\n",
    "        val_df['image_path'].tolist(),\n",
    "        val_df['findings'].tolist(),\n",
    "        tokenizer,\n",
    "        max_length=MAX_LENGTH,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\nüèóÔ∏è  Building model...\")\n",
    "    model = XRayReportGenerator(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_size=EMBED_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        use_lora=USE_LORA\n",
    "    ).to(device)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"   Total parameters:     {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    best_bleu = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nüìà Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, tokenizer, epoch)\n",
    "        print(f\"   Average Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\n   Evaluating model...\")\n",
    "        metrics = evaluate_model(model, val_loader, tokenizer, num_samples=20)\n",
    "        \n",
    "        print(f\"\\n   üìä Validation Metrics:\")\n",
    "        print(f\"      BLEU Score:  {metrics['BLEU']:.4f}\")\n",
    "        print(f\"      ROUGE-1:     {metrics['ROUGE-1']:.4f}\")\n",
    "        print(f\"      ROUGE-2:     {metrics['ROUGE-2']:.4f}\")\n",
    "        print(f\"      ROUGE-L:     {metrics['ROUGE-L']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if metrics['BLEU'] > best_bleu:\n",
    "            best_bleu = metrics['BLEU']\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'bleu': best_bleu,\n",
    "                'metrics': metrics\n",
    "            }, 'best_xray_model.pth')\n",
    "            print(f\"      ‚úÖ Best model saved! (BLEU: {best_bleu:.4f})\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best BLEU Score: {best_bleu:.4f} (Epoch {best_epoch})\")\n",
    "    \n",
    "    return model, tokenizer, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_report_from_path(image_path, model, tokenizer, transform):\n",
    "    \"\"\"Generate report from image path\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Generate report\n",
    "    report = model.generate_report(image, tokenizer, max_length=80)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def demo_report_generation(model, tokenizer, val_df, transform, num_examples=3):\n",
    "    \"\"\"Show example predictions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç Demo: Generating Reports for Sample Images\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(min(num_examples, len(val_df))):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        \n",
    "        image_path = val_df.iloc[i]['image_path']\n",
    "        actual_report = val_df.iloc[i]['findings']\n",
    "        \n",
    "        print(f\"Image: {os.path.basename(image_path)}\")\n",
    "        print(f\"\\nüìÑ Actual Report:\\n{actual_report[:200]}...\")\n",
    "        \n",
    "        generated_report = generate_report_from_path(image_path, model, tokenizer, transform)\n",
    "        print(f\"\\nü§ñ Generated Report:\\n{generated_report}\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    model, tokenizer, val_df = main()\n",
    "    \n",
    "    # Create transform for inference\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Show demo predictions\n",
    "    demo_report_generation(model, tokenizer, val_df, transform, num_examples=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Pipeline Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüí° To generate a report for a new image, use:\")\n",
    "    print(\"   report = generate_report_from_path(image_path, model, tokenizer, transform)\")\n",
    "    print(\"\\nüìÅ Best model saved as: best_xray_model.pth\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 516716,
     "sourceId": 951996,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
