{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-17T13:04:10.156221Z","iopub.execute_input":"2025-01-17T13:04:10.156497Z","iopub.status.idle":"2025-01-17T13:04:14.298172Z","shell.execute_reply.started":"2025-01-17T13:04:10.156468Z","shell.execute_reply":"2025-01-17T13:04:14.297493Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# train.py (Main Kaggle notebook file)\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport yaml\nimport logging\nfrom tqdm.notebook import tqdm\nimport wandb\nfrom datetime import datetime\nimport os\nimport math\n\n# Configuration\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'image_size': 224,\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01,\n        'warmup_steps': 100\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    \"\"\"\n    Prepare metadata by merging projections and reports data.\n    \"\"\"\n    # Read the CSV files\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    # Create metadata entries\n    metadata_entries = []\n    for _, row in projections_df.iterrows():\n        uid = row['uid']\n        filename = row['filename']\n        \n        # Get the corresponding report\n        report_data = reports_df[reports_df['uid'] == uid]\n        if len(report_data) == 0:\n            continue\n            \n        report_data = report_data.iloc[0]\n        \n        # Combine findings and impression for the report\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        \n        metadata_entries.append({\n            'image_filename': filename,\n            'report': report,\n            'projection': row['projection'],\n            'problems': report_data['Problems'],\n            'indication': report_data['indication']\n        })\n    \n    # Create metadata DataFrame\n    metadata_df = pd.DataFrame(metadata_entries)\n    \n    # Save the metadata to the working directory\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    print(f\"Metadata saved to {metadata_path}\")\n    \n    return metadata_path\n\n# Dataset class\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, transform=None, max_length=512):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        \n        # Load metadata from working directory\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n\n    def __len__(self):\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        # Update image path to use the correct subdirectory\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        report = row['report']\n        \n        # Load and transform image\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n            \n        # Process text\n        encoding = self.processor(\n            text=report,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n            legacy=False\n        )\n        \n        return {\n            'pixel_values': encoding['pixel_values'].squeeze(),\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'report': report\n        }\n\n# Model class with LoRA\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.lora_mappings = {}\n        # Add LoRA layers\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):\n        self.lora_layers = nn.ModuleDict()\n        for name, module in self.model.named_modules():\n            if \"query\" in name or \"value\" in name:\n                if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                    # Create a unique key for the layer\n                    layer_id = name.replace(\".\", \"_\")\n                    \n                    # Create LoRA layers\n                    lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                    lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                    \n                    # Initialize weights\n                    nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                    nn.init.zeros_(lora_up.weight)\n                    \n                    # Register the layers as ModuleList to ensure proper device placement and parameter tracking\n                    self.add_module(f\"lora_down_{layer_id}\", lora_down)\n                    self.add_module(f\"lora_up_{layer_id}\", lora_up)\n                    \n                    # Store the mapping between original module and its LoRA layers\n                    self.lora_mappings[name] = (f\"lora_down_{layer_id}\", f\"lora_up_{layer_id}\")\n    \n    def _freeze_base_model(self):\n        \"\"\"Freeze base model parameters and unfreeze LoRA parameters\"\"\"\n        for param in self.model.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze LoRA parameters\n        for down, up in self.lora_layers.values():\n            for param in down.parameters():\n                param.requires_grad = True\n            for param in up.parameters():\n                param.requires_grad = True\n    \n    def forward(self, images, input_ids=None, attention_mask=None, labels=None):\n        if pixel_values.requires_grad == False:\n            pixel_values = pixel_values.detach().requires_grad_(True)\n        outputs = self.model(\n            pixel_values=images,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True,\n            return_dict=True\n        )\n\n        hidden_states = outputs.hidden_states[-1]\n\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                if name in self.lora_layers:\n                    down_name, up_name = self.lora_mappings[name]\n                    lora_down = getattr(self, down_name)\n                    lora_up = getattr(self, up_name)\n                    \n                    # Apply LoRA transformation\n                    lora_output = lora_up(lora_down(hidden_states))\n                    \n                    # Combine with original output\n                    original_output = module(hidden_states)\n                    module.output = original_output + CONFIG['model']['lora_alpha'] * lora_output\n        \n        return outputs\n\n    def generate(self, images, max_length=100, num_beams=4):\n            \"\"\"Generate text based on image input\"\"\"\n            return self.model.generate(\n                pixel_values=images,\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True,\n                no_repeat_ngram_size=3\n            )\n\n     \n\n# Training function\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        # Training\n        model.train()\n        train_loss = 0\n        train_steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                # Move batch to device\n                pixel_values = batch['pixel_values'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                \n                # Forward pass\n                outputs = model(\n                    pixel_values=pixel_values,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                \n                loss = outputs.loss\n                \n                # Check for anomalous loss\n                if not torch.isfinite(loss):\n                    logging.warning(\"Non-finite loss detected, skipping batch\")\n                    continue\n                \n                # Backward pass with gradient clipping\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_steps += 1\n                \n                # Update progress bar\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    if hasattr(torch.cuda, 'empty_cache'):\n                        torch.cuda.empty_cache()\n                    logging.warning(\"GPU OOM, skipping batch\")\n                    continue\n                raise e\n        \n        avg_train_loss = train_loss / train_steps\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        val_steps = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validating'):\n                images = batch['image'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                \n                outputs = model(\n                    images=images,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                \n                val_loss += outputs.loss.item()\n                val_steps += 1\n        \n        avg_val_loss = val_loss / val_steps\n        \n        # Log metrics\n        print(f'Epoch {epoch + 1}:')\n        print(f'Average training loss: {avg_train_loss:.4f}')\n        print(f'Average validation loss: {avg_val_loss:.4f}')\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'val_loss': best_val_loss\n            }, '/kaggle/working/best_model_lora.pt')\n\ndef main():\n    #input_path = CONFIG['data']['input_path']\n    \n    prepare_metadata(\n        input_path=CONFIG[\"data\"][\"input_path\"],\n        output_path=CONFIG[\"data\"][\"output_path\"]\n    )\n    # Set up transforms\n    transform = transforms.Compose([\n        transforms.Resize((CONFIG['data']['image_size'], CONFIG['data']['image_size'])),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    # Create dataset\n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        transform=transform,\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    # Split dataset\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        dataset, [train_size, val_size]\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    # Initialize model\n    model = XRayReportGenerator()\n    \n    # Train model\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:34:11.294347Z","iopub.execute_input":"2025-01-17T14:34:11.294755Z","iopub.status.idle":"2025-01-17T14:34:17.535131Z","shell.execute_reply.started":"2025-01-17T14:34:11.294728Z","shell.execute_reply":"2025-01-17T14:34:17.533395Z"}},"outputs":[{"name":"stdout","text":"Metadata saved to /kaggle/working/metadata.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3410c94b14f4459a820fec093c1b142"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-db388528e34d>\u001b[0m in \u001b[0;36m<cell line: 366>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-db388528e34d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-db388528e34d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;31m# Move batch to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-16-db388528e34d>\", line 120, in __getitem__\n    'pixel_values': encoding['pixel_values'].squeeze(),\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\", line 87, in __getitem__\n    return self.data[item]\nKeyError: 'pixel_values'\n"],"ename":"KeyError","evalue":"Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-16-db388528e34d>\", line 120, in __getitem__\n    'pixel_values': encoding['pixel_values'].squeeze(),\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py\", line 87, in __getitem__\n    return self.data[item]\nKeyError: 'pixel_values'\n","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'image_size': 224,\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01,\n        'warmup_steps': 100\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n            \n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        \n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, transform=None, max_length=512):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        \n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        \n        # Process both image and text using the processor\n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Remove batch dimension added by processor\n        for k in encoding.keys():\n            encoding[k] = encoding[k].squeeze(0)\n            \n        return encoding\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        hidden_states = outputs.hidden_states[-1]\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    module.weight.data += (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        return outputs\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        train_loss = 0\n        train_steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        print(f'Epoch {epoch + 1}:')\n        print(f'Average training loss: {train_loss/train_steps:.4f}')\n        \n        if train_loss/train_steps < best_val_loss:\n            best_val_loss = train_loss/train_steps\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_val_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    transform = transforms.Compose([\n        transforms.Resize((CONFIG['data']['image_size'], CONFIG['data']['image_size'])),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        transform=transform,\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:40:03.641962Z","iopub.execute_input":"2025-01-17T14:40:03.642404Z","iopub.status.idle":"2025-01-17T14:40:08.714967Z","shell.execute_reply.started":"2025-01-17T14:40:03.642367Z","shell.execute_reply":"2025-01-17T14:40:08.712673Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec2a1818ee5486fa2583ec76795cd35"}},"metadata":{}},{"name":"stderr","text":"Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, if both images and text are provided, the last token (EOS token) of the input_ids and attention_mask tensors will be removed. To test the new behavior, set `legacy=False`as a processor call argument.\nLegacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, if both images and text are provided, the last token (EOS token) of the input_ids and attention_mask tensors will be removed. To test the new behavior, set `legacy=False`as a processor call argument.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-a4ca362ddd07>\u001b[0m in \u001b[0;36m<cell line: 237>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-a4ca362ddd07>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXRayReportGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-a4ca362ddd07>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-17-a4ca362ddd07>\", line 78, in __getitem__\n    encoding = self.processor(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/git/processing_git.py\", line 124, in __call__\n    image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 325, in preprocess\n    image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 191, in resize\n    return resize(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 337, in resize\n    do_rescale = _rescale_for_pil_conversion(image)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 158, in _rescale_for_pil_conversion\n    raise ValueError(\nValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1007792949676514, 2.6051416397094727] which cannot be converted to uint8.\n"],"ename":"ValueError","evalue":"Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-17-a4ca362ddd07>\", line 78, in __getitem__\n    encoding = self.processor(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/git/processing_git.py\", line 124, in __call__\n    image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 325, in preprocess\n    image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 191, in resize\n    return resize(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 337, in resize\n    do_rescale = _rescale_for_pil_conversion(image)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 158, in _rescale_for_pil_conversion\n    raise ValueError(\nValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1007792949676514, 2.6051416397094727] which cannot be converted to uint8.\n","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, max_length=512):\n        self.data_dir = Path(data_dir)\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        image = Image.open(image_path).convert('RGB')\n        \n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n            legacy=False\n        )\n        \n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        hidden_states = outputs.hidden_states[-1]\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    module.weight.data += (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        return outputs\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        total_loss = 0\n        steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                total_loss += loss.item()\n                steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        avg_loss = total_loss / steps\n        print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:41:49.203897Z","iopub.execute_input":"2025-01-17T14:41:49.204296Z","iopub.status.idle":"2025-01-17T14:41:56.860945Z","shell.execute_reply.started":"2025-01-17T14:41:49.204260Z","shell.execute_reply":"2025-01-17T14:41:56.859516Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5917f521f2497ea72ce8047ba6ad1d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1c3926739d73>\u001b[0m in \u001b[0;36m<cell line: 217>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-1c3926739d73>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXRayReportGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1c3926739d73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1c3926739d73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 outputs = model(\n\u001b[0m\u001b[1;32m    152\u001b[0m                     \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1c3926739d73>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0mlora_down\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0mlora_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_down\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lora_alpha'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlora_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'image_size': 224,\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01,\n        'warmup_steps': 100\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n            \n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        \n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, transform=None, max_length=512):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        \n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        \n        # Process both image and text using the processor\n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Remove batch dimension added by processor\n        for k in encoding.keys():\n            encoding[k] = encoding[k].squeeze(0)\n            \n        return encoding\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        hidden_states = outputs.hidden_states[-1]\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    module.weight.data += (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        return outputs\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        train_loss = 0\n        train_steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        print(f'Epoch {epoch + 1}:')\n        print(f'Average training loss: {train_loss/train_steps:.4f}')\n        \n        if train_loss/train_steps < best_val_loss:\n            best_val_loss = train_loss/train_steps\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_val_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    transform = transforms.Compose([\n        transforms.Resize((CONFIG['data']['image_size'], CONFIG['data']['image_size'])),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        transform=transform,\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T14:42:10.603726Z","iopub.execute_input":"2025-01-17T14:42:10.604086Z","iopub.status.idle":"2025-01-17T14:42:15.598663Z","shell.execute_reply.started":"2025-01-17T14:42:10.604060Z","shell.execute_reply":"2025-01-17T14:42:15.596711Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c760dcdbaa64431da6f67c55b1be96fb"}},"metadata":{}},{"name":"stderr","text":"Legacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, if both images and text are provided, the last token (EOS token) of the input_ids and attention_mask tensors will be removed. To test the new behavior, set `legacy=False`as a processor call argument.\nLegacy behavior is being used. The current behavior will be deprecated in version 5.0.0. In the new behavior, if both images and text are provided, the last token (EOS token) of the input_ids and attention_mask tensors will be removed. To test the new behavior, set `legacy=False`as a processor call argument.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-a4ca362ddd07>\u001b[0m in \u001b[0;36m<cell line: 237>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-a4ca362ddd07>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXRayReportGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-a4ca362ddd07>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-19-a4ca362ddd07>\", line 78, in __getitem__\n    encoding = self.processor(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/git/processing_git.py\", line 124, in __call__\n    image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 325, in preprocess\n    image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 191, in resize\n    return resize(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 337, in resize\n    do_rescale = _rescale_for_pil_conversion(image)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 158, in _rescale_for_pil_conversion\n    raise ValueError(\nValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1007792949676514, 2.640000104904175] which cannot be converted to uint8.\n"],"ename":"ValueError","evalue":"Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-19-a4ca362ddd07>\", line 78, in __getitem__\n    encoding = self.processor(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/git/processing_git.py\", line 124, in __call__\n    image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 41, in __call__\n    return self.preprocess(images, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 325, in preprocess\n    image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/image_processing_clip.py\", line 191, in resize\n    return resize(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 337, in resize\n    do_rescale = _rescale_for_pil_conversion(image)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 158, in _rescale_for_pil_conversion\n    raise ValueError(\nValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1007792949676514, 2.640000104904175] which cannot be converted to uint8.\n","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, max_length=512):\n        self.data_dir = Path(data_dir)\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        image = Image.open(image_path).convert('RGB')\n        \n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n            legacy=False\n        )\n        \n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        hidden_states = outputs.hidden_states[-1]\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    \n                    # Align dimensions if there is a mismatch\n                    if lora_output.size(1) != module.weight.size(1):\n                        lora_output = lora_output[:, :module.weight.size(1)]\n                    \n                    module.weight.data += (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        return outputs\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        total_loss = 0\n        steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                total_loss += loss.item()\n                steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        avg_loss = total_loss / steps\n        print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\n    print(f\"Hidden states shape: {hidden_states.shape}\")\n    print(f\"LoRA output shape: {lora_output.shape}\")\n    print(f\"Module weight shape: {module.weight.shape}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T15:31:21.574336Z","iopub.execute_input":"2025-01-17T15:31:21.574711Z","iopub.status.idle":"2025-01-17T15:31:29.262219Z","shell.execute_reply.started":"2025-01-17T15:31:21.574680Z","shell.execute_reply":"2025-01-17T15:31:29.260700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef1ca756eb244d0ad410de5b962b196"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-279873968d6f>\u001b[0m in \u001b[0;36m<cell line: 226>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-279873968d6f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXRayReportGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hidden states shape: {hidden_states.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-279873968d6f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-279873968d6f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 outputs = model(\n\u001b[0m\u001b[1;32m    157\u001b[0m                     \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-279873968d6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    128\u001b[0m                         \u001b[0mlora_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lora_alpha'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlora_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():  # Fixed variable name and added underscore\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, max_length=512):  # Fixed method name\n        self.data_dir = Path(data_dir)\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):  # Fixed method name\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):  # Fixed method name\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        image = Image.open(image_path).convert('RGB')\n        \n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n            legacy=False\n        )\n        \n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):  # Fixed method name\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):  # Fixed method name\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):  # Fixed method name\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        # Store original weights\n        original_weights = {}\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    original_weights[layer_id] = module.weight.data.clone()\n                    \n                    # Apply LoRA updates\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    hidden_states = outputs.hidden_states[-1]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    \n                    if lora_output.shape[-1] != module.weight.shape[-1]:\n                        lora_output = lora_output[..., :module.weight.shape[-1]]\n                    \n                    module.weight.data = original_weights[layer_id] + (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        # Get final output with modified weights\n        final_output = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        # Restore original weights\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in original_weights:\n                    module.weight.data = original_weights[layer_id]\n        \n        return final_output\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        total_loss = 0\n        steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                total_loss += loss.item()\n                steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        avg_loss = total_loss / steps\n        print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":  # Fixed conditional\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport math\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nCONFIG = {\n    'data': {\n        'input_path': '/kaggle/input/chest-xrays-indiana-university',\n        'output_path': '/kaggle/working',\n        'image_dir': 'images/images_normalized',\n        'max_length': 512,\n        'batch_size': 8\n    },\n    'model': {\n        'name': 'microsoft/git-base',\n        'lora_rank': 4,\n        'lora_alpha': 32\n    },\n    'training': {\n        'epochs': 10,\n        'learning_rate': 2e-5,\n        'weight_decay': 0.01\n    }\n}\n\ndef prepare_metadata(input_path, output_path):\n    projections_df = pd.read_csv(os.path.join(input_path, 'indiana_projections.csv'))\n    reports_df = pd.read_csv(os.path.join(input_path, 'indiana_reports.csv'))\n    \n    metadata_entries = []\n    for _, row in projections_df.iterrows():  # Fixed variable name and added underscore\n        report_data = reports_df[reports_df['uid'] == row['uid']]\n        if len(report_data) == 0:\n            continue\n        report_data = report_data.iloc[0]\n        report = f\"Findings: {report_data['findings']} Impression: {report_data['impression']}\"\n        metadata_entries.append({\n            'image_filename': row['filename'],\n            'report': report,\n        })\n    \n    metadata_df = pd.DataFrame(metadata_entries)\n    metadata_path = os.path.join(output_path, 'metadata.csv')\n    metadata_df.to_csv(metadata_path, index=False)\n    return metadata_path\n\nclass XRayReportDataset(Dataset):\n    def __init__(self, data_dir, max_length=512):  # Fixed method name\n        self.data_dir = Path(data_dir)\n        self.processor = AutoProcessor.from_pretrained(CONFIG['model']['name'])\n        self.max_length = max_length\n        self.metadata = pd.read_csv(Path(CONFIG['data']['output_path']) / 'metadata.csv')\n    \n    def __len__(self):  # Fixed method name\n        return len(self.metadata)\n        \n    def __getitem__(self, idx):  # Fixed method name\n        row = self.metadata.iloc[idx]\n        image_path = self.data_dir / CONFIG['data']['image_dir'] / row['image_filename']\n        image = Image.open(image_path).convert('RGB')\n        \n        encoding = self.processor(\n            images=image,\n            text=row['report'],\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n            legacy=False\n        )\n        \n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\nclass XRayReportGenerator(nn.Module):\n    def __init__(self):  # Fixed method name\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(CONFIG['model']['name'])\n        self.lora_layers = nn.ModuleDict()\n        self._add_lora_layers()\n        self._freeze_base_model()\n    \n    def _add_lora_layers(self):  # Fixed method name\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                lora_down = nn.Linear(module.in_features, CONFIG['model']['lora_rank'], bias=False)\n                lora_up = nn.Linear(CONFIG['model']['lora_rank'], module.out_features, bias=False)\n                \n                nn.init.kaiming_uniform_(lora_down.weight, a=math.sqrt(5))\n                nn.init.zeros_(lora_up.weight)\n                \n                self.lora_layers[layer_id] = nn.ModuleList([lora_down, lora_up])\n    \n    def _freeze_base_model(self):  # Fixed method name\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        for layer in self.lora_layers.values():\n            for param in layer.parameters():\n                param.requires_grad = True\n    \n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            output_hidden_states=True\n        )\n        \n        # Store original weights\n        original_weights = {}\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in self.lora_layers:\n                    original_weights[layer_id] = module.weight.data.clone()\n                    \n                    # Apply LoRA updates\n                    lora_down, lora_up = self.lora_layers[layer_id]\n                    hidden_states = outputs.hidden_states[-1]\n                    lora_output = lora_up(lora_down(hidden_states))\n                    \n                    if lora_output.shape[-1] != module.weight.shape[-1]:\n                        lora_output = lora_output[..., :module.weight.shape[-1]]\n                    \n                    module.weight.data = original_weights[layer_id] + (CONFIG['model']['lora_alpha'] * lora_output)\n        \n        # Get final output with modified weights\n        final_output = self.model(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        # Restore original weights\n        for name, module in self.model.named_modules():\n            if isinstance(module, nn.Linear) and any(x in name for x in [\"query\", \"value\"]):\n                layer_id = name.replace(\".\", \"_\")\n                if layer_id in original_weights:\n                    module.weight.data = original_weights[layer_id]\n        \n        return final_output\n\ndef train_model(model, train_loader, val_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=CONFIG['training']['learning_rate'],\n        weight_decay=CONFIG['training']['weight_decay']\n    )\n    \n    best_loss = float('inf')\n    \n    for epoch in range(CONFIG['training']['epochs']):\n        model.train()\n        total_loss = 0\n        steps = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n        for batch in progress_bar:\n            try:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                \n                outputs = model(\n                    pixel_values=batch['pixel_values'],\n                    input_ids=batch['input_ids'],\n                    attention_mask=batch['attention_mask'],\n                    labels=batch['input_ids']\n                )\n                \n                loss = outputs.loss\n                if not torch.isfinite(loss):\n                    continue\n                \n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                total_loss += loss.item()\n                steps += 1\n                progress_bar.set_postfix({'loss': loss.item()})\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    torch.cuda.empty_cache()\n                    continue\n                raise e\n        \n        avg_loss = total_loss / steps\n        print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save({\n                'lora_state_dict': model.lora_layers.state_dict(),\n                'epoch': epoch,\n                'loss': best_loss\n            }, os.path.join(CONFIG['data']['output_path'], 'best_model_lora.pt'))\n\ndef main():\n    prepare_metadata(CONFIG['data']['input_path'], CONFIG['data']['output_path'])\n    \n    dataset = XRayReportDataset(\n        data_dir=CONFIG['data']['input_path'],\n        max_length=CONFIG['data']['max_length']\n    )\n    \n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['data']['batch_size'],\n        shuffle=False,\n        num_workers=2\n    )\n    \n    model = XRayReportGenerator()\n    train_model(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":  # Fixed conditional\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T16:59:22.828150Z","iopub.execute_input":"2025-01-17T16:59:22.828399Z","iopub.status.idle":"2025-01-17T16:59:56.748732Z","shell.execute_reply.started":"2025-01-17T16:59:22.828377Z","shell.execute_reply":"2025-01-17T16:59:56.747463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898799acad7c4f12b597957e3777bebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccb090e606264392a26891abcf569176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984bef934bf34eafb86ac3499f2810e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8799fb729a48a5879db750e3575f00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c52961ed732c476490fbe93210f6e7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce6c4a14c144eeb83813958ab993e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/707M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f65008cd614e2abbb09051af8b793b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e3174ffc4e4af2b1d9c6ab9d124f51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/747 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d7ca1e896a0450ca2466a45a6f8c5b0"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9d603a19b345>\u001b[0m in \u001b[0;36m<cell line: 240>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Fixed conditional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-9d603a19b345>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXRayReportGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Fixed conditional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9d603a19b345>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9d603a19b345>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 outputs = model(\n\u001b[0m\u001b[1;32m    175\u001b[0m                     \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9d603a19b345>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    131\u001b[0m                         \u001b[0mlora_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lora_alpha'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlora_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Get final output with modified weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1"],"ename":"RuntimeError","evalue":"The size of tensor a (768) must match the size of tensor b (708) at non-singleton dimension 1","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}